## AI大模型的概述

### 1、AI大模型的基本概念

**如下图所示**

<img src="/img/rag/ai大模型基本概念.png" alt="" />

> - LLM的知识不是实时的
> - LLM可能不知道你私有的领域/业务知识


### 提供API

`DeepSeek-R1 - model.py`

```py
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import unicodedata
from typing import List

@torch.inference_mode()
def generate(
    model: AutoModelForCausalLM,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    max_new_tokens: int,
    temperature: float = 1.0
) -> List[int]:
    """
    Generate response from the model with attention_mask provided.
    """
    outputs = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,  # 提供显式 attention mask
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        eos_token_id=model.config.eos_token_id,
        pad_token_id=model.config.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
    )
    return outputs[0].tolist()

def clean_input(user_input):
    """
    清理用户输入，去除不可见字符和多余的空格。
    """
    user_input = "".join(c for c in user_input if not unicodedata.category(c).startswith("C"))  # 移除控制字符
    return user_input.strip()  # 去除首尾空格

def clean_message_content(content):
    """
    清理消息内容，去除首尾空格并过滤非法输入
    """
    if not content or not isinstance(content, str):
        return ""
    return content.strip()  # 去除首尾空格

def build_prompt(messages, max_history=3):
    """
    Build prompt for the model, limiting the history to the most recent messages.
    """
    template = "The following is a conversation with an AI assistant. The assistant is helpful, knowledgeable, and polite:\n"
    for msg in messages[-max_history:]:
        content = clean_message_content(msg["content"])
        if not content:  # 跳过空内容
            continue
        template += f"{msg['role'].capitalize()}: {content}\n"
    template += "Assistant: "
    return template.strip()  # 确保返回值是字符串

def generate_response(text):
    print("服务启动中...")

    ckpt_path = "/root/DeepSeek-R1"

    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)
    model = AutoModelForCausalLM.from_pretrained(
        ckpt_path,
        torch_dtype=torch.bfloat16,
    ).cuda()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    messages = []

    messages.append({"role": "user", "content": text})

    messages = messages[-10:]  # 只保留最近 10 条对话

    # Build prompt and tokenize
    prompt = build_prompt(messages)

    tokenized_prompt = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
    input_ids = tokenized_prompt["input_ids"].to("cuda")
    attention_mask = tokenized_prompt["attention_mask"].to("cuda")

    # Generate response
    max_new_tokens = 150
    temperature = 0.7

    completion_tokens = generate(model, input_ids, attention_mask, max_new_tokens, temperature)
    completion = tokenizer.decode(
        completion_tokens[len(input_ids[0]):],  # 从输入长度截取生成部分
        skip_special_tokens=True
    ).split("User:")[0].strip()

    messages.append({"role": "assistant", "content": completion})

    return f"机器人说话：{completion}"
```

`Flask - main.py`

```py
from flask import Flask, request, jsonify
from model import generate_response

app = Flask(__name__)

@app.route('/generate', methods=['GET'])
def generate():
    user_input = request.args.get('user_input')

    response = generate_response(user_input)
    
    return jsonify({"response": response})

if __name__ == '__main__':
    app.run(debug=True)

```